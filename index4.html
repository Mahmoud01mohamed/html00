<html>
    <title>
        Artificial Intellegance
    </title>
    <head>
        
          <center> <img src="html.12img.jpg" height="500" width="500"></center> 
           
   <p>
    <h3>
        بدأ تاريخ الذكاء الاصطناعي في العصور القديمة، من خلال الأساطير والقصص والشائعات عن الكائنات الاصطناعية الموهوبة بالذكاء أو الوعي من قبل الحرفيين المهرة. زُرعت بذور الذكاء الاصطناعي الحديث من قبل الفلاسفة الكلاسيكيين الذين حاولوا وصف عملية التفكير الإنساني بأنها عبارة عن التلاعب الميكانيكي للرموز. تُوج هذا العمل باختراع الكمبيوتر الرقمي القابل للبرمجة في الأربعينيات من القرن العشرين، وهي آلة تعتمد على جوهر التفكير المنطقي الرياضي. ألهم هذا الجهاز والأفكار التي تقف وراءه حفنة من العلماء للبدء بجدية في مناقشة إمكانية بناء الدماغ الإلكتروني.
تأسس مجال أبحاث الذكاء الاصطناعى ضمن ورشة عمل في حرم كلية دارتموث خلال صيف عام 1956م. أولئك الذين حضروا سيصبحون قادة لأبحاث الذكاء الاصطناعى لعدة عقود. تنبأ العديد منهم بأن آلة بذكاء الإنسان لن تكون موجودة في أكثر من جيل، وأنهم حصلوا على ملايين الدولارات لجعل هذه الرؤية حقيقة.[1]
في النهاية، أصبح من الواضح أنهم قللوا بشكل كبير من صعوبة المشروع. في عام 1973م، استجابةً لانتقادات جيمس لايتهيل والضغط المستمر من الكونغرس، أوقفت الحكومتان الأمريكية والبريطانية تمويل البحوث غير الموجهة في مجال الذكاء الاصطناعي، وستُعرف السنوات الصعبة التي تلت ذلك باسم «شتاء الذكاء الاصطناعي». بعد سبع سنوات، ألهمت المبادرة اليابانية التي تبنتها الحكومة اليابانية، الحكومات والصناعة لتزويد مشاريع الذكاء الاصطناعي بمليارات الدولارات، ولكن بحلول أواخر الثمانينيات أُصيب المستثمرون بخيبة أمل بسبب عدم وجود الطاقة اللازمة للكمبيوتر (الآلات) وسحبوا التمويل مرةً أخرى.
ازدهر الاستثمار والاهتمام بالذكاء الاصطناعى في العقود الأولى من القرن الحادي والعشرين، عندما طُبقت عملية تعلم الآلة بنجاح على العديد من المشكلات في الأوساط الأكاديمية والصناعية بسبب الأساليب الجديدة، وطُبقت أجهزة الكمبيوتر القوية، وجُمعت مجموعات ضخمة من البيانات.
 
التعلم العميق والبيانات الضخمة والذكاء العام المصطنع: منذ عام 2011 حتى الآن
في العقود الأولى من القرن الحادي والعشرين، طُبق بنجاح الوصول إلى كميات كبيرة من البيانات (المعروفة باسم «البيانات الضخمة»)، وأجهزة الكمبيوتر الأرخص والأسرع، وتقنيات تعلم الآلة المتقدمة على العديد من المشاكل في جميع الأنظمة الاقتصادية. في الواقع، قدّر معهد ماكنزي العالمي في مقالته الشهيرة «البيانات الضخمة: الحدود التالية للابتكار والمنافسة والإنتاجية» أنه «بحلول عام 2009، كان لدى جميع القطاعات تقريبًا في الاقتصاد الأمريكي ما يعادل 200 تيرابايت على الأقل من معدل البيانات المخزنة».
بحلول عام 2016، وصل سوق المنتجات والأجهزة والبرامج المرتبطة بـالذكاء الاصطناعي إلى أكثر من 8 مليارات دولار، وذكرت صحيفة نيويورك تايمز أن الاهتمام بـالذكاء الاصطناعي وصل إلى حد «الجنون». بدأت تطبيقات البيانات الضخمة في الوصول إلى مجالات أخرى أيضًا، مثل نماذج التدريب في علم البيئة وللتطبيقات المختلفة في الاقتصاد. أدى التقدم في التعلم العميق (وخاصةً الشبكات العصبونية الالتفافية العميقة والشبكات العصبونية المتكررة) إلى التقدم والبحث في معالجة الصور والفيديو، وتحليل النص، وحتى التعرف على الكلام. 
التعلم العميق
التعلم العميق هو فرع من التعلم الآلي يصور نماذج تجريدية عالية المستوى في البيانات باستخدام رسم بياني عميق يحتوي على العديد من مستويات المعالجة. وفقًا لمبرهنة التقريب العام، فإن العمق ليس ضروريًا لتكون الشبكة العصبية قادرة على التقريب بين الوظائف العشوائية المستمرة. ومع ذلك، هناك العديد من المشكلات الشائعة للشبكات السطحية (مثل المطابقة) التي تساعد الشبكات العميقة على تجنبها. على هذا النحو، فإن الشبكات العصبونية العميقة قادرة على توليد نماذج أكثر تعقيدًا بشكل واقعي مقارنةً بنظيراتها السطحية.[6]
ومع ذلك، فإن التعلم العميق لديه مشاكل خاصة به. هناك مشكلة شائعة في الشبكات العصبونية المتكررة هي مشكلة تلاشي معدل الانحدار، حيث يتقلص معدل الانحدار بين الطبقات تدريجيًا ويختفي حرفيًا حيث يُقرّب إلى الصفر. هناك العديد من الطرق التي طُورت للتعامل مع هذه المشكلة، مثل وحدات الذاكرة قصيرة المدى المطولة.
يمكن أن تنافس بنى الشبكات العصبونية العميقة المتطورة في بعض الأحيان دقة الإنسان في مجالات مثل تصور الكمبيوتر، وتحديدًا في أمور مثل قاعدة بيانات إم إن آي إس تي والتعرف على إشارات المرور. 
يمكن لمحركات معالجة اللغة التي تعمل بمحركات البحث الذكية، التغلب بسهولة على البشر في الإجابة على الأسئلة السخيفة العامة (مثل نظام حاسوب آي بي إم واتسون)، وأحدثت التطورات الحديثة في التعليم العميق نتائج مذهلة في التنافس مع البشر، في أشياء مثل لعبة غو آند دوم (وهي لعبة أن يفوز أول شخص يطلق النار، التي أثارت بعض الجدل). 
البيانات الضخمة
تشير البيانات الضخمة إلى مجموعة من البيانات التي لا يمكن التقاطها وإدارتها ومعالجتها بواسطة أدوات البرامج التقليدية في إطار زمني معين. إنها كمية هائلة من قدرات صنع القرار والبصيرة وتحسين العمليات التي تتطلب نماذج معالجة جديدة. في عصر البيانات الضخمة بيغ داتا إيرا الذي كتبه فيكتور ماير شونبيرغ وكينيث كوك، تعني البيانات الضخمة أنه بدلًا من التحليل العشوائي (باستخدام عينة مسح)، تُستخدم جميع البيانات للتحليل. خصائص البيانات الضخمة (5 فولت) (التي اقترحتها شركة آي بي إم): الحجم والسرعة والتنوع والقيمة والدقة. إن الأهمية الاستراتيجية لتكنولوجيا البيانات الضخمة لا تتمثل في إتقان معلومات البيانات الضخمة، ولكن تخصصها في هذه البيانات المهمة. بمعنى آخر، إذا شُبهت البيانات الضخمة بصناعة ما، فإن مفتاح تحقيق الربحية في هذه الصناعة هو زيادة «قدرة معالجة» البيانات وتحقيق «القيمة الإضافية» للبيانات من خلال «المعالجة».
الذكاء العام الاصطناعي
الذكاء الاصطناعي هو فرع من فروع علوم الكمبيوتر يحاول فهم جوهر الذكاء وإنتاج آلة ذكية جديدة تستجيب بطريقة مشابهة للذكاء البشري. تشمل الأبحاث في هذا المجال الروبوتات والتعرف على الكلام والتعرف على الصور ومعالجة اللغة الطبيعية وأنظمة الخبراء. منذ ولادة الذكاء الاصطناعي، أصبحت النظرية والتقنية أكثر نضجًا، وتوسعت مجالات التطبيق. من المُتوقع أن تكون المنتجات التكنولوجية التي جلبها الذكاء الاصطناعي في المستقبل عبارة عن «حاوية» للمعرفة الإنسانية. ويمكن للذكاء الاصطناعي أن يحاكي عملية المعلومات المتعلقة بالوعي والتفكير البشري. الذكاء الاصطناعي ليس ذكاءً بشريًا، لكنه يمكن أن يكون مشابه للتفكير البشري، ويمكن أن يتجاوز الذكاء البشري. يُشار أيضًا إلى الذكاء العام الاصطناعي باسم «الذكاء الاصطناعي القوي»، أو «الذكاء الاصطناعي الكامل» أو كقدرة الآلة على تنفيذ «الأداء الذكي العام». تحتفظ المصادر الأكاديمية على «الذكاء الاصطناعى القوي» للإشارة إلى الآلات القادرة على الشعور بالوعي
 


    </h3>
   </p>
      
</html>